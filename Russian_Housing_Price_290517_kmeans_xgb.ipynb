{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection, preprocessing\n",
    "import xgboost as xgb\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import TransformerMixin\n",
    "import math\n",
    "from sklearn import datasets, linear_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macro_cols = [\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\",\n",
    "\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n",
    "\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]\n",
    "\n",
    "## https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317/comments/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'F:/Kaggle/Sberbank Housing Price Prediction'\n",
    "train = pd.read_csv(os.path.join(path,\"train.csv\"),parse_dates=['timestamp'])\n",
    "test = pd.read_csv(os.path.join(path,\"test.csv\"),parse_dates=['timestamp'])\n",
    "macro = pd.read_csv(os.path.join(path,\"macro.csv\"),parse_dates=['timestamp'], usecols=['timestamp'] + macro_cols)\n",
    "submission = pd.read_csv(os.path.join(path,\"sample_submission.csv\"))\n",
    "\n",
    "train = pd.merge(train, macro, how='left', on='timestamp')\n",
    "test = pd.merge(test, macro, how='left', on='timestamp')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup right CV strategy using https://www.kaggle.com/c/sberbank-russian-housing-market/discussion/32717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trainsub = train[train.timestamp < '2015-01-01']\n",
    "trainsub = trainsub[trainsub.product_type==\"Investment\"]\n",
    "\n",
    "ind_1m = trainsub[trainsub.price_doc <= 1000000].index\n",
    "ind_2m = trainsub[trainsub.price_doc == 2000000].index\n",
    "ind_3m = trainsub[trainsub.price_doc == 3000000].index\n",
    "\n",
    "train_index = set(train.index.copy())\n",
    "\n",
    "for ind, gap in zip([ind_1m, ind_2m, ind_3m], [10, 3, 2]):\n",
    "    ind_set = set(ind)\n",
    "    ind_set_cut = ind.difference(set(ind[::gap]))\n",
    "\n",
    "    train_index = train_index.difference(ind_set_cut)\n",
    "\n",
    "train = train.loc[train_index]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### drop cols with all NAs they dont contribute in any case and take intersection b/w test and train data\n",
    "test.dropna(how=\"all\", axis=1)\n",
    "train.dropna(how=\"all\", axis=1)\n",
    "\n",
    "col_test = test.columns\n",
    "col_train = train.columns\n",
    "col_train_unique = set(col_train)\n",
    "intersection = [val for val in col_test if val in col_train_unique]\n",
    "#print (intersection)\n",
    "\n",
    "price_doc = train['price_doc']\n",
    "train = train[intersection]\n",
    "test = test[intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_set = pd.concat([train,test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hmean(x):\n",
    "    try:\n",
    "        return stats.hmean(x)\n",
    "    except:\n",
    "        return 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Feature engg \n",
    "### need to incorporate more features -- floor/full_sq, life_sq/full_sq,build_month (new), build_week (new),kitch/full, kitch>full_sq,\n",
    "### harmonic mean and mean of (school, water, kindergarten, fitness, public_transport_station_km,church_synagogue,metro, metro_min_walk)\n",
    "\n",
    "combined_set['floor_by_full'] = combined_set['floor']/combined_set['full_sq']\n",
    "combined_set['kitch_by_full'] = combined_set['kitch_sq']/combined_set['full_sq']\n",
    "combined_set['life_by_full'] = combined_set['life_sq']/combined_set['full_sq']\n",
    "combined_set['avg_area_per_room'] = combined_set['full_sq']/combined_set['num_room']\n",
    "\n",
    "absolutely_good_stuff = ['bus_terminal_avto_km','fitness_km','green_zone_km','office_km','school_km','workplaces_km','public_healthcare_km']\n",
    "educational_stuff  = ['additional_education_km','kindergarten_km','preschool_km','university_km']\n",
    "religious_stuff = ['big_church_km','church_synagogue_km','mosque_km']\n",
    "work_stuff = ['office_km','workplaces_km']\n",
    "fitness_health_stuff = ['basketball_km','fitness_km','green_zone_km','park_km','public_healthcare_km']\n",
    "bad_stuff = ['detention_facility_km','hospice_morgue_km','industrial_km','nuclear_reactor_km',]\n",
    "other_good_stuff  = ['bus_terminal_avto_km','public_transport_station_km','railroad_station_avto_km']\n",
    "\n",
    "combined_set['absolutely_good_stuff'] = combined_set[absolutely_good_stuff].apply(hmean, axis=1)\n",
    "combined_set['max_good_stuff'] = combined_set[absolutely_good_stuff].apply(max, axis=1)\n",
    "combined_set['min_good_stuff'] = combined_set[absolutely_good_stuff].apply(min, axis=1)\n",
    "combined_set['religious_stuff'] = combined_set[religious_stuff].apply(hmean, axis=1)\n",
    "combined_set['max_religious_stuff'] = combined_set[religious_stuff].apply(max, axis=1)\n",
    "combined_set['min_religious_stuff'] = combined_set[religious_stuff].apply(min, axis=1)\n",
    "combined_set['work_stuff'] = combined_set[work_stuff].apply(hmean, axis=1)\n",
    "combined_set['max_work_stuff'] = combined_set[work_stuff].apply(max, axis=1)\n",
    "combined_set['min_work_stuff'] = combined_set[work_stuff].apply(min, axis=1)\n",
    "combined_set['fitness_health_stuff'] = combined_set[fitness_health_stuff].apply(hmean, axis=1)\n",
    "combined_set['max_fitness_health_stuff'] = combined_set[fitness_health_stuff].apply(max, axis=1)\n",
    "combined_set['min_fitness_health_stuff'] = combined_set[fitness_health_stuff].apply(min, axis=1)\n",
    "\n",
    "combined_set['min_bad_stuff'] = combined_set[bad_stuff].apply(min, axis=1)\n",
    "combined_set['bad_stuff_hmean'] = combined_set[bad_stuff].apply(hmean, axis=1)\n",
    "combined_set['max_bad_stuff'] = combined_set[bad_stuff].apply(min, axis=1)\n",
    "\n",
    "combined_set['other_good_stuff'] = combined_set[other_good_stuff].apply(hmean, axis=1)\n",
    "combined_set['min_other_good_stuff'] = combined_set[other_good_stuff].apply(min, axis=1)\n",
    "combined_set['max_other_good_stuff'] = combined_set[other_good_stuff].apply(max, axis=1)\n",
    "\n",
    "\n",
    "# Add month-year\n",
    "month_year = (combined_set.timestamp.dt.month + combined_set.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "combined_set['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (combined_set.timestamp.dt.weekofyear + combined_set.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "combined_set['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "# Add month and day-of-week\n",
    "combined_set['month'] = combined_set.timestamp.dt.month\n",
    "combined_set['dow'] = combined_set.timestamp.dt.dayofweek\n",
    "\n",
    "# Remove timestamp column (may overfit the model in train)\n",
    "combined_set.drop(['timestamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We are working towards a K-means based clustering followed by 2 stage price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.base import TransformerMixin\n",
    "## missing value impute \n",
    "\n",
    "def imputer (dataframe,impute_continuous = True, impute_categorical=True):\n",
    "    categorical = []\n",
    "    continuous = []\n",
    "    for f in dataframe.columns:\n",
    "        if dataframe[f].dtype=='object':\n",
    "            categorical.append(f)\n",
    "        else:\n",
    "            continuous.append(f)\n",
    "            \n",
    "    print (categorical)\n",
    "    print (continuous)\n",
    "    if (impute_continuous):\n",
    "        for c in continuous:\n",
    "            if pd.isnull(dataframe[c].mean()):\n",
    "                fill = -99\n",
    "                dataframe[c].fillna(fill,inplace=True)\n",
    "                print (\"filling -99 for \",c)\n",
    "            else:\n",
    "                dataframe[c].fillna(dataframe[c].mean(), inplace=True)\n",
    "                print (c+\"_mean is_\" + str(dataframe[c].mean()))\n",
    "                print (\"-------------------------\")\n",
    "    \n",
    "    print (\"===============================================\")\n",
    "    \n",
    "    if (impute_categorical):\n",
    "        for c in categorical:\n",
    "            if pd.isnull(dataframe[c].mode()[0]):\n",
    "                fill = -99\n",
    "                dataframe[c].fillna(fill,inplace=True)\n",
    "                print (\"filling -99 for \",c)\n",
    "            else:\n",
    "                dataframe[c].fillna(dataframe[c].mode()[0],inplace=True)\n",
    "                print (c+\"_mode is_\" + str(dataframe[c].mode()[0]))\n",
    "                print (\"-------------------------\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_set = combined_set.replace(np.inf, 0)\n",
    "combined_set_upd = imputer (dataframe=combined_set,impute_continuous = True, impute_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### separarte test and train data sets now -- these are final test/train sets -- still need to setup right CV test data\n",
    "final_train = combined_set_upd[:train.shape[0]] # Up to the last initial training set row\n",
    "final_test = combined_set_upd[train.shape[0]:] # Past the last initial training set row\n",
    "\n",
    "print ('shape of train is:', final_train.shape)\n",
    "print ('shape of test is:',final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_var =[]\n",
    "for f in final_train.ix[:, final_train.columns != 'timestamp'].columns:\n",
    "    if final_train[f].dtype=='object':\n",
    "        print(f)\n",
    "        categorical_var.append(f)\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(final_train[f].values.astype('str')) + list(final_test[f].values.astype('str')))\n",
    "        final_train[f] = lbl.transform(list(final_train[f].values.astype('str')))\n",
    "        final_test[f] = lbl.transform(list(final_test[f].values.astype('str')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = price_doc\n",
    "final_train.drop('id',axis=1,inplace=True)\n",
    "final_test.drop('id',axis=1,inplace=True)\n",
    "X_test = final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### define the rmsle func\n",
    "def rmsle_eval(y, y0):\n",
    "    y0=y0.get_label()    \n",
    "    assert len(y) == len(y0)\n",
    "    return 'error',np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_columns = list(set(final_train.select_dtypes(include=['float64', 'int64']).columns) - set(['id', 'timestamp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMEANS FROM HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need z std for each column\n",
    "from scipy.stats import zscore\n",
    "\n",
    "final_train.apply(zscore)\n",
    "final_test.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = price_doc.values\n",
    "x_train = final_train[train_columns].values\n",
    "x_test = final_test[train_columns].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### look at the variance decomposition using the elbow plot for different values of k (k-means)\n",
    "import numpy as np\n",
    "from scipy import cluster\n",
    "from matplotlib import pyplot\n",
    "initial = [cluster.vq.kmeans(x_train,i) for i in range(1,10)]\n",
    "pyplot.plot([var for (cent,var) in initial])\n",
    "pyplot.show()\n",
    "\n",
    "## takeaway: 7 clusters look good for starters -- although 7 is a bit too much, but we will persist and check later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clusters = 5\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=0).fit(x_train)\n",
    "kmeans.labels_\n",
    "test_labels = kmeans.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Now that we have n clusters --- we will do separate prediction for each using xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#29035 / 7662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### separarte test and train data sets now -- these are final test/train sets -- still need to setup right CV test data\n",
    "final_train = combined_set[:train.shape[0]] # Up to the last initial training set row\n",
    "final_test = combined_set[train.shape[0]:] # Past the last initial training set row\n",
    "\n",
    "print ('shape of train is:', final_train.shape)\n",
    "print ('shape of test is:',final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### encoding \n",
    "categorical_var =[]\n",
    "for f in final_train.ix[:, final_train.columns != 'timestamp'].columns:\n",
    "    if final_train[f].dtype=='object':\n",
    "        print(f)\n",
    "        categorical_var.append(f)\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(final_train[f].values.astype('str')) + list(final_test[f].values.astype('str')))\n",
    "        final_train[f] = lbl.transform(list(final_train[f].values.astype('str')))\n",
    "        final_test[f] = lbl.transform(list(final_test[f].values.astype('str')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reindex the train/test cluster labels to merge later on\n",
    "assign = final_train.index.values\n",
    "train_labels = pd.DataFrame({\"cluster\":kmeans.labels_})\n",
    "#train_labels = train_labels.set_index[assign]\n",
    "train_labels.index = assign\n",
    "#train_labels\n",
    "\n",
    "assign1 = final_test.index.values\n",
    "test_labels = pd.DataFrame({\"cluster\":test_labels})\n",
    "#train_labels = train_labels.set_index[assign]\n",
    "test_labels.index = assign1\n",
    "#test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## merge test/train sets with respective labels\n",
    "final_train = pd.concat([final_train,train_labels],axis=1)\n",
    "final_test = pd.concat([final_test,test_labels],axis=1)\n",
    "\n",
    "### So, at this point we have both test and train datasets appended with respective clusters\n",
    "\n",
    "#### next: create separate test/train for each cluster and train a xgb separately \n",
    "#### automate this so that we can iterate over number of clusters\n",
    "\n",
    "#### for those clusters where prediction is not good, we will use the global xgb estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### real stuff from here \n",
    "\n",
    "## remember there is still a y_train --- which is price_doc from original train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### subset test/train based on n-clusters --- this will give n dataframes --- still wokring on how to name these dfs properly\n",
    "clusters = clusters\n",
    "train_names =[]\n",
    "for cluster in range(clusters):\n",
    "    train_names.append(\"train\"+str(cluster))\n",
    "train_names    \n",
    "#name = {}\n",
    "for cluster in range(clusters):\n",
    "    name = (train_names[cluster])\n",
    "    print (name)\n",
    "    train_names[cluster] = final_train[final_train['cluster'] == cluster]\n",
    "    \n",
    "test_names =[]\n",
    "for cluster in range(clusters):\n",
    "    test_names.append(\"test\"+str(cluster))\n",
    "test_names    \n",
    "#name = {}\n",
    "for cluster in range(clusters):\n",
    "    name = (test_names[cluster])\n",
    "    print (name)\n",
    "    test_names[cluster] = final_test[final_test['cluster'] == cluster]    \n",
    "    ### the n dataframes are named train_names[0], train_names[1] and so on till clusters-1 ....--- need to fix this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we will now do a PCA/SNE based dimensionality redxn to try and plot all vars and price_doc to check visualization of clusters\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "components = 5\n",
    "pca = PCA(n_components=components)\n",
    "pca.fit(x_train)\n",
    "PCA(copy=True, iterated_power='auto', n_components=components, random_state=None,svd_solver='auto', tol=0.0, whiten=False)\n",
    "print(pca.explained_variance_ratio_) \n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "#plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')\n",
    "\n",
    "## this is a weird looking graph --- not at all smooth\n",
    "## so, we only need only 1 components from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pca with only 1 component\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(x_train)\n",
    "PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,svd_solver='auto', tol=0.0, whiten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### visualilze the PCA0 and price_doc based on cluster values for train set\n",
    "\n",
    "PCA0 = pca.transform(x_train) \n",
    "#y_train\n",
    "#final_train['cluster']\n",
    "\n",
    "assign_upd = final_train.index.values\n",
    "y_train = pd.DataFrame(y_train)\n",
    "PCA0 = pd.DataFrame(PCA0)\n",
    "y_train.index = assign_upd\n",
    "PCA0.index = assign_upd\n",
    "\n",
    "for_viz = pd.concat([pd.DataFrame(final_train['cluster']),pd.DataFrame(y_train),pd.DataFrame(PCA0)],axis=1)\n",
    "for_viz.columns = ['cluster','price_doc','PCA0']\n",
    "for_viz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f, ax = plt.subplots(figsize=(7, 7))\n",
    "#ax.set(xscale=\"log\")\n",
    "plot = sns.lmplot(x=\"PCA0\", y=\"price_doc\", hue=\"cluster\",fit_reg=False, data=for_viz)\n",
    "plot.set(yscale=\"log\")\n",
    "\n",
    "### takeaway: within each cluster price varies quite a bit but pca0 is more or less static \n",
    "### this could be because we did not factor in price during clustering process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### prepare separate dfs for each cluster --- the train will entail further train+33% valid sets\n",
    "\n",
    "train_c0 = final_train.loc[train_names[0].index]\n",
    "train_c1 = final_train.loc[train_names[1].index]\n",
    "train_c2 = final_train.loc[train_names[2].index]\n",
    "train_c3 = final_train.loc[train_names[3].index]\n",
    "train_c4 = final_train.loc[train_names[4].index]\n",
    "\n",
    "train_c0.drop(['id','cluster'],axis=1, inplace=True)\n",
    "train_c1.drop(['id','cluster'],axis=1, inplace=True)\n",
    "train_c2.drop(['id','cluster'],axis=1, inplace=True)\n",
    "train_c3.drop(['id','cluster'],axis=1, inplace=True)\n",
    "train_c4.drop(['id','cluster'],axis=1, inplace=True)\n",
    "\n",
    "#train_c1.drop('id',axis=1, inplace=True)\n",
    "#train_c2.drop('id',axis=1, inplace=True)\n",
    "#train_c3.drop('id',axis=1, inplace=True)\n",
    "#train_c4.drop('id',axis=1, inplace=True)\n",
    "\n",
    "test_c0 = final_test.loc[test_names[0].index]\n",
    "test_c1 = final_test.loc[test_names[1].index]\n",
    "test_c2 = final_test.loc[test_names[2].index]\n",
    "test_c3 = final_test.loc[test_names[3].index]\n",
    "test_c4 = final_test.loc[test_names[4].index]\n",
    "\n",
    "\n",
    "#test_c0.drop('id',axis=1, inplace=True)\n",
    "#test_c1.drop('id',axis=1, inplace=True)\n",
    "#test_c2.drop('id',axis=1, inplace=True)\n",
    "#test_c3.drop('id',axis=1, inplace=True)\n",
    "#test_c4.drop('id',axis=1, inplace=True)\n",
    "\n",
    "y_train_c0 = y_train.loc[train_names[0].index]\n",
    "y_train_c1 = y_train.loc[train_names[1].index]\n",
    "y_train_c2 = y_train.loc[train_names[2].index]\n",
    "y_train_c3 = y_train.loc[train_names[3].index]\n",
    "y_train_c4 = y_train.loc[train_names[4].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgb for c0\n",
    "\n",
    "test_size = 0.33\n",
    "\n",
    "X_tr_c0, X_test_c0, Y_tr_c0, Y_test_c0 = train_test_split(train_c0, y_train_c0, test_size=test_size, random_state=1234)\n",
    "#X_tr_c1, X_test_c1, Y_tr_c1, Y_test_c1 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c2, X_test_c2, Y_tr_c2, Y_test_c2 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c3, X_test_c3, Y_tr_c3, Y_test_c3 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c4, X_test_c4, Y_tr_c4, Y_test_c4 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "\n",
    "xg_train = xgb.DMatrix(X_tr_c0, Y_tr_c0.values)\n",
    "xg_valid = xgb.DMatrix(X_test_c0, Y_test_c0.values)\n",
    "test_c0_upd = test_c0.drop(['id','cluster'],axis=1,inplace=False)\n",
    "xg_test = xgb.DMatrix(test_c0_upd)\n",
    "\n",
    "param ={}\n",
    "param['objective'] = 'reg:linear'\n",
    "param[\"subsample\"] = 0.9\n",
    "param[\"colsample_bytree\"] = 0.7\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['lambda'] = 1\n",
    "param['min_child_weight'] = 3\n",
    "#param['eval_metric'] = ['rmse']\n",
    "#param['eval_metric'] = [rmsle_eval]\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_valid, 'test') ]\n",
    "num_round = 10000\n",
    "\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist,feval=rmsle_eval,early_stopping_rounds=30)\n",
    "pred_for_rmsle = bst.predict(xg_valid)\n",
    "\n",
    "num_boost_round = bst.best_iteration\n",
    "bst = xgb.train(param, xg_train, num_boost_round,watchlist)\n",
    "prediction = bst.predict(xg_test)\n",
    "df_c0 = pd.DataFrame({'id':test_c0.id,'price_doc':prediction})\n",
    "#df.to_csv('fresh_try1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgb for c1\n",
    "\n",
    "test_size = 0.33\n",
    "\n",
    "#X_tr_c0, X_test_c0, Y_tr_c0, Y_test_c0 = train_test_split(train_c0, y_train_c0, test_size=test_size, random_state=1234)\n",
    "X_tr_c1, X_test_c1, Y_tr_c1, Y_test_c1 = train_test_split(train_c1, y_train_c1, test_size=test_size, random_state=1234)\n",
    "#X_tr_c2, X_test_c2, Y_tr_c2, Y_test_c2 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c3, X_test_c3, Y_tr_c3, Y_test_c3 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c4, X_test_c4, Y_tr_c4, Y_test_c4 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "\n",
    "xg_train = xgb.DMatrix(X_tr_c1, Y_tr_c1.values)\n",
    "xg_valid = xgb.DMatrix(X_test_c1, Y_test_c1.values)\n",
    "test_c1_upd = test_c1.drop(['id','cluster'],axis=1,inplace=False)\n",
    "xg_test = xgb.DMatrix(test_c1_upd)\n",
    "\n",
    "param ={}\n",
    "param['objective'] = 'reg:linear'\n",
    "param[\"subsample\"] = 0.9\n",
    "param[\"colsample_bytree\"] = 0.7\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['lambda'] = 1\n",
    "param['min_child_weight'] = 3\n",
    "#param['eval_metric'] = ['rmse']\n",
    "#param['eval_metric'] = [rmsle_eval]\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_valid, 'test') ]\n",
    "num_round = 10000\n",
    "\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist,feval=rmsle_eval,early_stopping_rounds=30)\n",
    "pred_for_rmsle = bst.predict(xg_valid)\n",
    "\n",
    "num_boost_round = bst.best_iteration\n",
    "bst = xgb.train(param, xg_train, num_boost_round,watchlist)\n",
    "prediction = bst.predict(xg_test)\n",
    "df_c1 = pd.DataFrame({'id':test_c1.id,'price_doc':prediction})\n",
    "#df.to_csv('fresh_try1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgb for c2\n",
    "\n",
    "test_size = 0.33\n",
    "\n",
    "#X_tr_c0, X_test_c0, Y_tr_c0, Y_test_c0 = train_test_split(train_c0, y_train_c0, test_size=test_size, random_state=1234)\n",
    "#X_tr_c1, X_test_c1, Y_tr_c1, Y_test_c1 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "X_tr_c2, X_test_c2, Y_tr_c2, Y_test_c2 = train_test_split(train_c2, y_train_c2, test_size=test_size, random_state=1234)\n",
    "#X_tr_c3, X_test_c3, Y_tr_c3, Y_test_c3 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c4, X_test_c4, Y_tr_c4, Y_test_c4 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "\n",
    "xg_train = xgb.DMatrix(X_tr_c2, Y_tr_c2.values)\n",
    "xg_valid = xgb.DMatrix(X_test_c2, Y_test_c2.values)\n",
    "test_c2_upd = test_c2.drop(['id','cluster'],axis=1,inplace=False)\n",
    "xg_test = xgb.DMatrix(test_c2_upd)\n",
    "\n",
    "param ={}\n",
    "param['objective'] = 'reg:linear'\n",
    "param[\"subsample\"] = 0.9\n",
    "param[\"colsample_bytree\"] = 0.7\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['lambda'] = 1\n",
    "param['min_child_weight'] = 3\n",
    "#param['eval_metric'] = ['rmse']\n",
    "#param['eval_metric'] = [rmsle_eval]\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_valid, 'test') ]\n",
    "num_round = 10000\n",
    "\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist,feval=rmsle_eval,early_stopping_rounds=30)\n",
    "pred_for_rmsle = bst.predict(xg_valid)\n",
    "\n",
    "num_boost_round = bst.best_iteration\n",
    "bst = xgb.train(param, xg_train, num_boost_round,watchlist)\n",
    "prediction = bst.predict(xg_test)\n",
    "df_c2 = pd.DataFrame({'id':test_c2.id,'price_doc':prediction})\n",
    "#df.to_csv('fresh_try1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgb for c3\n",
    "\n",
    "test_size = 0.33\n",
    "\n",
    "#X_tr_c0, X_test_c0, Y_tr_c0, Y_test_c0 = train_test_split(train_c0, y_train_c0, test_size=test_size, random_state=1234)\n",
    "#X_tr_c1, X_test_c1, Y_tr_c1, Y_test_c1 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c2, X_test_c2, Y_tr_c2, Y_test_c2 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "X_tr_c3, X_test_c3, Y_tr_c3, Y_test_c3 = train_test_split(train_c3, y_train_c3, test_size=test_size, random_state=1234)\n",
    "#X_tr_c4, X_test_c4, Y_tr_c4, Y_test_c4 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "\n",
    "xg_train = xgb.DMatrix(X_tr_c3, Y_tr_c3.values)\n",
    "xg_valid = xgb.DMatrix(X_test_c3, Y_test_c3.values)\n",
    "test_c3_upd = test_c3.drop(['id','cluster'],axis=1,inplace=False)\n",
    "xg_test = xgb.DMatrix(test_c3_upd)\n",
    "\n",
    "param ={}\n",
    "param['objective'] = 'reg:linear'\n",
    "param[\"subsample\"] = 0.9\n",
    "param[\"colsample_bytree\"] = 0.7\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['lambda'] = 1\n",
    "param['min_child_weight'] = 3\n",
    "#param['eval_metric'] = ['rmse']\n",
    "#param['eval_metric'] = [rmsle_eval]\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_valid, 'test') ]\n",
    "num_round = 10000\n",
    "\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist,feval=rmsle_eval,early_stopping_rounds=30)\n",
    "pred_for_rmsle = bst.predict(xg_valid)\n",
    "\n",
    "num_boost_round = bst.best_iteration\n",
    "bst = xgb.train(param, xg_train, num_boost_round,watchlist)\n",
    "prediction = bst.predict(xg_test)\n",
    "df_c3 = pd.DataFrame({'id':test_c3.id,'price_doc':prediction})\n",
    "#df.to_csv('fresh_try1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgb for c4\n",
    "\n",
    "test_size = 0.33\n",
    "\n",
    "#X_tr_c0, X_test_c0, Y_tr_c0, Y_test_c0 = train_test_split(train_c0, y_train_c0, test_size=test_size, random_state=1234)\n",
    "#X_tr_c1, X_test_c1, Y_tr_c1, Y_test_c1 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c2, X_test_c2, Y_tr_c2, Y_test_c2 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "#X_tr_c3, X_test_c3, Y_tr_c3, Y_test_c3 = train_test_split(x_train, y_train, test_size=test_size, random_state=1234)\n",
    "X_tr_c4, X_test_c4, Y_tr_c4, Y_test_c4 = train_test_split(train_c4, y_train_c4, test_size=test_size, random_state=1234)\n",
    "\n",
    "xg_train = xgb.DMatrix(X_tr_c4, Y_tr_c4.values)\n",
    "xg_valid = xgb.DMatrix(X_test_c4, Y_test_c4.values)\n",
    "test_c4_upd = test_c4.drop(['id','cluster'],axis=1,inplace=False)\n",
    "xg_test = xgb.DMatrix(test_c4_upd)\n",
    "\n",
    "param ={}\n",
    "param['objective'] = 'reg:linear'\n",
    "param[\"subsample\"] = 0.9\n",
    "param[\"colsample_bytree\"] = 0.7\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['lambda'] = 1\n",
    "param['min_child_weight'] = 3\n",
    "#param['eval_metric'] = ['rmse']\n",
    "#param['eval_metric'] = [rmsle_eval]\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_valid, 'test') ]\n",
    "num_round = 10000\n",
    "\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist,feval=rmsle_eval,early_stopping_rounds=30)\n",
    "pred_for_rmsle = bst.predict(xg_valid)\n",
    "\n",
    "num_boost_round = bst.best_iteration\n",
    "bst = xgb.train(param, xg_train, num_boost_round,watchlist)\n",
    "prediction = bst.predict(xg_test)\n",
    "df_c4 = pd.DataFrame({'id':test_c4.id,'price_doc':prediction})\n",
    "#df.to_csv('fresh_try1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([df_c0,df_c1,df_c2,df_c3,df_c4],axis=0)\n",
    "submit.sort_index(axis=0,inplace=True)\n",
    "submit.tail()\n",
    "submit.to_csv(\"kmeans_XGB_try1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
